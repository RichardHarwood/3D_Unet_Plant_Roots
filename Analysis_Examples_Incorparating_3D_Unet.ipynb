{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0AQ9ynAXnKEymi2YSpxLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardHarwood/3D_Unet_Plant_Roots/blob/main/Analysis_Examples_Incorparating_3D_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYPQeqGgIzzv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wget\n",
        "!pip install open3d\n",
        "!pip install vedo\n",
        "#!pip install stackview ipycanvas==0.11\n",
        "import os\n",
        "!git clone https://github.com/wolny/pytorch-3dunet.git\n",
        "%cd pytorch-3dunet\n",
        "!python setup.py install\n",
        "!pip install imagecodecs\n",
        "!pip install skan\n",
        "!pip install porespy\n",
        "!pip install pypardiso\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQgFMqRQLv0i",
        "outputId": "624d45c4-2198-4caa-8418-0052eb023b42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "bNgIjXSOLv3g",
        "outputId": "d520fcc8-ddfc-4fbf-b9ea-84d2600fe465"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to set appropriate  home folder\n",
        "import os.path\n",
        "from os import path\n",
        "if path.exists('/content/drive/MyDrive/3D_UNET_analysis_eg/') == False:\n",
        "  os.mkdir('/content/drive/MyDrive/3D_UNET_analysis_eg/')\n",
        "home_folder = \"/content/drive/MyDrive/3D_UNET_analysis_eg/\""
      ],
      "metadata": {
        "id": "MKgtmgigLv6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Config Folder\n",
        "if path.exists(home_folder+'config_files') == False:\n",
        "  os.mkdir(home_folder+'config_files')\n",
        "config_folder = home_folder+'config_files/'"
      ],
      "metadata": {
        "id": "Ecu8FvPmL7Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Checkpoint Folder\n",
        "if path.exists(home_folder+'checkpoint_dir') == False:\n",
        "  os.mkdir(home_folder+'checkpoint_dir')\n",
        "checkpoint_dir = home_folder+'checkpoint_dir/'"
      ],
      "metadata": {
        "id": "iWozoAAQL7Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Folder to store images we want to segment\n",
        "if path.exists(home_folder+'segment_this_folder') == False:\n",
        "  os.mkdir(home_folder+'segment_this_folder')\n",
        "imgs_to_seg_dir = home_folder+'segment_this_folder/'"
      ],
      "metadata": {
        "id": "Cat7todrL7bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download image to test workflow on\n",
        "can be slow depending on image, using a very small one here to illustrate the workflow"
      ],
      "metadata": {
        "id": "CyZ1iZr3OTXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import wget\n",
        "if os.path.isfile(imgs_to_seg_dir+'root_unet_eg.tif'):\n",
        "  print(\"image already download\")\n",
        "else:\n",
        "  wget.download(\"https://zenodo.org/records/14189395/files/\"+\"recon_W1_WW_S1_R3.tif\"+\"?download=1\", out=imgs_to_seg_dir+'root_unet_eg.tif')\n",
        "\n"
      ],
      "metadata": {
        "id": "hWj-ZZDrL7d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to H5\n",
        "import skimage\n",
        "import imagecodecs\n",
        "import h5py\n",
        "from scipy.ndimage import zoom\n",
        "#need to reduce image due to RAM depending on what resources you have\n",
        "scale_factor=0.2\n",
        "img=skimage.io.imread(imgs_to_seg_dir+'root_unet_eg.tif')\n",
        "img=zoom(img, (scale_factor,scale_factor, scale_factor), order=1)\n",
        "hf = h5py.File(imgs_to_seg_dir+\"root_unet_eg.h5\", 'w')\n",
        "hf.create_dataset('raw', data=img)\n",
        "hf.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "KyE9qWznPjZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downlaod model to segment image\n"
      ],
      "metadata": {
        "id": "cvywMDjIOX2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(checkpoint_dir+'3D_Unet_Root_Model.pytorch'):\n",
        "  print(\"model already download\")\n",
        "else:\n",
        "  wget.download(\"https://zenodo.org/records/13958667/files/\"+\"3D_Unet_Root_Model.pytorch\"+\"?download=1\", out=checkpoint_dir+'3D_Unet_Root_Model.pytorch')\n"
      ],
      "metadata": {
        "id": "gvBM2L9YOWOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"UNet3D\"\n",
        "in_channels = 1\n",
        "out_channels= 1\n",
        "layer_order = \"gcr\"\n",
        "f_maps = [32,\n",
        "          64,\n",
        "          128,\n",
        "          256]\n",
        "num_groups = 8\n",
        "final_sigmoid = True\n",
        "is_segmentation = True\n",
        "batch_size= 2  #important for gpu memory\n",
        "num_workers= 2\n",
        "raw_internal_path= \"raw\"\n",
        "label_internal_path= \"label\"\n",
        "weight_internal_path= None\n",
        "######################\n",
        "slice_builder_name= \"SliceBuilder\"\n",
        "slice_builder_name_predict = \"SliceBuilder\"\n",
        "patch_shape= [40, 170, 170]  #Change depending on GPU memory\n",
        "stride_shape= [20, 40, 40]   #Change depending on GPU memory\n",
        "halo_shape= [16, 32, 32]\n",
        "threshold = 0.01\n",
        "slack_acceptance = 0.01\n",
        "name_transformer= \"Standardize\"\n",
        "name_transformer_label= \"BlobsToMask\"\n",
        "append_label= False\n",
        "boundary= False\n",
        "ToTensor_name = \"ToTensor\"\n",
        "expand_dims_false= False\n",
        "expand_dims_true= True\n",
        "#####################\n",
        "predictor_name='StandardPredictor'\n",
        "#################################\n",
        "file_paths_test= imgs_to_seg_dir\n",
        "#############################\n",
        "model_path=checkpoint_dir+'3D_Unet_Root_Model.pytorch'"
      ],
      "metadata": {
        "id": "YFPggACkLv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "test_config_yaml ={\n",
        "    \"model_path\": model_path,\n",
        "    \"model\": {\n",
        "        \"name\": model_name,\n",
        "        \"in_channels\": in_channels,\n",
        "        \"out_channels\": out_channels,\n",
        "        \"layer_order\": layer_order,\n",
        "        \"f_maps\": f_maps,\n",
        "        \"num_groups\": num_groups,\n",
        "        \"final_sigmoid\": final_sigmoid,\n",
        "        \"is_segmentation\": is_segmentation\n",
        "    },\n",
        "    \"predictor\": {\n",
        "        \"name\": predictor_name\n",
        "    },\n",
        "    \"loaders\": {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"raw_internal_path\": raw_internal_path,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"test\": {\n",
        "            \"file_paths\": [\n",
        "                file_paths_test\n",
        "            ],\n",
        "            \"slice_builder\": {\n",
        "                \"name\": slice_builder_name_predict,\n",
        "                \"patch_shape\": patch_shape,\n",
        "                \"stride_shape\": patch_shape,\n",
        "                \"halo_shape\": halo_shape\n",
        "            },\n",
        "            \"transformer\": {\n",
        "                \"raw\": [\n",
        "                    {\n",
        "                        \"name\": name_transformer\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": ToTensor_name,\n",
        "                        \"expand_dims\": expand_dims_true\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(config_folder+'predict.yaml', 'w') as yaml_file:\n",
        "    yaml.dump(test_config_yaml, yaml_file, default_flow_style=False, sort_keys=False)\n"
      ],
      "metadata": {
        "id": "AFK0jDeSLwAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!predict3dunet --config /content/drive/MyDrive/3D_UNET_analysis_eg/config_files/predict.yaml"
      ],
      "metadata": {
        "id": "NIFySZtULwCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "raw_data_path= file_paths_test\n",
        "raw_file_names = [os.path.basename(x) for x in glob.glob(imgs_to_seg_dir+\"*.h5\")]\n",
        "raw_file_names = [e for e in raw_file_names if \"predictions\" in e ]\n",
        "#################################\n",
        "if path.exists(home_folder+'processed/') == False:\n",
        "  os.mkdir(home_folder+'processed/')\n",
        "file_paths_processed= home_folder+'processed/'\n",
        "#########################\n",
        "#raw_file_names\n",
        "\n",
        "UNIQUE_ID =np.unique(raw_file_names)\n",
        "len(UNIQUE_ID)"
      ],
      "metadata": {
        "id": "OqIVNlYfLwFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNIQUE_ID"
      ],
      "metadata": {
        "id": "7BWuOh5KUFHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scale_factor=0.5\n",
        "Z=6.5*2\n",
        "Y=6.5*2\n",
        "X=6.5*2"
      ],
      "metadata": {
        "id": "mncZ_U0cTwhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import cv2\n",
        "import skimage\n",
        "from skimage.filters import threshold_otsu, threshold_yen,threshold_li\n",
        "from skimage.measure import label\n",
        "from skimage.morphology import remove_small_objects\n",
        "from vedo import *\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in UNIQUE_ID:\n",
        "    print(i)\n",
        "    hf=h5py.File(raw_data_path+i)\n",
        "    hf.keys()\n",
        "    dataset_IMGs= hf['predictions']\n",
        "    unet_prediction=dataset_IMGs[:]\n",
        "    unet_prediction=unet_prediction[0,:,:,:]\n",
        "    #unet_prediction=zoom(unet_prediction, (scale_factor,scale_factor, scale_factor), order=1)\n",
        "    unet_prediction = cv2.normalize(unet_prediction, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    thresh = threshold_li(unet_prediction)\n",
        "    root_binary = unet_prediction > thresh\n",
        "\n",
        "    root_binary_clean=label(root_binary)\n",
        "    min_size_coef=150000/4\n",
        "    root_binary_clean = remove_small_objects(root_binary_clean, min_size=min_size_coef, connectivity=2)\n",
        "    root_binary_clean = (root_binary_clean != 0).astype(bool)\n",
        "\n",
        "\n",
        "    root_VOL = Volume(root_binary_clean, spacing=((1),(1),(1)))\n",
        "    root_ISO = root_VOL.isosurface(1)\n",
        "    root_ISO.write(file_paths_processed+i+\"__root.stl\")\n",
        "    skimage.io.imsave(file_paths_processed+i+\"_processed.tif\", root_binary_clean, check_contrast=False)\n",
        "    plt.imshow(root_binary_clean[30])"
      ],
      "metadata": {
        "id": "5T1btd9ATlQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stls love crashing colab for some reason (PyVitsa and Vedo) so just surface plot the root to inspect\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "binary_image=root_binary_clean.astype(int)\n",
        "x, y, z = np.indices(binary_image.shape)\n",
        "x_vals, y_vals, z_vals = x[binary_image == 1], y[binary_image == 1], z[binary_image == 1]\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x_vals, y_vals, z_vals, c='r', marker='o')\n",
        "ax.set_xlabel('X axis')\n",
        "ax.set_ylabel('Y axis')\n",
        "ax.set_zlabel('Z axis')\n",
        "ax.set_title('3D Root Image Surface Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3_ZNNlH9Ut7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example one: Extract the skeleton\n",
        "https://skeleton-analysis.org/stable/index.html"
      ],
      "metadata": {
        "id": "dMDWEibYXd5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#package import not sorted correctly, kind of done for each example\n",
        "import numpy as np\n",
        "import skan\n",
        "from skimage.data import binary_blobs\n",
        "from skimage.morphology import skeletonize\n",
        "import scipy.ndimage as ndi\n",
        "\n",
        "#root_binary_clean_sml=zoom(root_binary_clean, (0.5,0.5, 0.5), order=0)\n",
        "binary_skeleton = skeletonize(root_binary_clean)\n",
        "skeleton = skan.Skeleton(binary_skeleton)\n",
        "\n",
        "all_paths = [\n",
        "        skeleton.path_coordinates(i)\n",
        "        for i in range(skeleton.n_paths)\n",
        "        ]\n",
        "\n",
        "paths_table = skan.summarize(skeleton, separator='_')\n",
        "paths_table['path_id'] = np.arange(skeleton.n_paths)\n",
        "paths_table['random_path_id'] = np.random.default_rng().permutation(skeleton.n_paths)*6.6 #need to tweak this for vis to work\n",
        "paths_table.head()\n"
      ],
      "metadata": {
        "id": "kw21mcKWXOJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example two: Extract the pore network"
      ],
      "metadata": {
        "id": "e3xfY4naZZO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import porespy as ps\n",
        "import openpnm as op\n",
        "import scipy.ndimage as spim\n",
        "from porespy.filters import find_peaks, trim_saddle_points, trim_nearby_peaks\n",
        "from porespy.tools import randomize_colors\n",
        "from scipy import ndimage as ndi\n",
        "from skimage.segmentation import watershed\n",
        "import seaborn as sns\n",
        "#Dilate around the root to make a rhizosheath\n",
        "diamond = ndi.generate_binary_structure(rank=3, connectivity=1)\n",
        "iternation_n=20\n",
        "dilated = ndi.binary_dilation(root_binary_clean, diamond, iterations=iternation_n)\n",
        "rhizosheath= root_binary_clean ^ dilated\n",
        "#Get Pore Network\n",
        "ct_data=zoom(img, (scale_factor,scale_factor, scale_factor), order=1)\n",
        "pore_thresh = threshold_otsu(ct_data)\n",
        "pore= ct_data < pore_thresh\n",
        "#Get Pore Network in Rhizosheath\n",
        "pore_rhizo=pore * rhizosheath\n",
        "\n",
        "#######################################\n",
        "sigma = 0.4\n",
        "dt = spim.distance_transform_edt(input=pore_rhizo)\n",
        "dt1 = spim.gaussian_filter(input=dt, sigma=sigma)\n",
        "peaks = find_peaks(dt=dt)\n",
        "print('Initial number of peaks: ', spim.label(peaks)[1])\n",
        "peaks = trim_saddle_points(peaks=peaks, dt=dt1)\n",
        "print('Peaks after trimming saddle points: ', spim.label(peaks)[1])\n",
        "peaks = trim_nearby_peaks(peaks=peaks, dt=dt)\n",
        "peaks, N = spim.label(peaks)\n",
        "print('Peaks after trimming nearby peaks: ', N)\n",
        "regions = watershed(image=-dt, markers=peaks, mask=dt > 0)\n",
        "regions = randomize_colors(regions)\n",
        "############################\n",
        "mat_slice=int(len(rhizosheath)/2)\n",
        "plt.figure(figsize= (5,5));\n",
        "plt.imshow(rhizosheath[mat_slice]);\n",
        "plt.title(\"rhizosheath\");\n",
        "plt.show();\n",
        "\n",
        "import pandas as pd\n",
        "import imageio\n",
        "net = ps.networks.regions_to_network(regions*pore_rhizo, voxel_size=X)\n",
        "pn = op.io.network_from_porespy(net)\n",
        "\n",
        "pore_region_volume_df=pd.DataFrame(pn['pore.region_volume'])\n",
        "throat_cons_df=pd.DataFrame(pn['throat.conns'])\n",
        "\n",
        "#saves data for visualising, cant be donw in colab, check our parraview\n",
        "#pore_rhizo = (np.swapaxes(pore_rhizo, 2, 0)) # This isjust some of ps.tools.align_image_with_openpnm(im), it works to align image with\n",
        "imageio.volsave(file_paths_processed+'pore_networkfor_rhizosheath.tif', np.array(pore_rhizo, dtype=np.int8))\n",
        "op.io.project_to_vtk(project=pn.project,filename=(file_paths_processed+\"pore_network\"))\n",
        "\n",
        "#sns.kdeplot(data=pore_region_volume_df, x=0).set_title(\"Spead of region pore size\");\n",
        "\n",
        "fig, ax = plt.subplots(figsize=[5, 5])\n",
        "#ax.imshow(slice_2d.T, cmap=plt.cm.bone);\n",
        "op.visualization.plot_coordinates(ax=fig,\n",
        "                                  network=pn,\n",
        "                                  size_by=pn[\"pore.region_volume\"],\n",
        "                                  color_by=pn[\"throat.total_length\"],\n",
        "                                  markersize=50)\n",
        "op.visualization.plot_connections(network=pn, ax=fig)\n",
        "ax.set_title('Pore Network in the rhizosheath')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "c5F2zi4UXOSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interact with the pore network\n",
        "op.visualization.plot_notebook(network=pn, node_color=0, edge_color=0, node_size=1, node_scale=20, edge_scale=5, colormap='viridis')"
      ],
      "metadata": {
        "id": "mY6w0wB5zGL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example three: Visualize  root density\n",
        "Pretty underwhelming with this example"
      ],
      "metadata": {
        "id": "WzCll3wdeNgd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlIabnyDfDcW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}